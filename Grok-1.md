# Grok-1

**314B parameters Mixture-of-Experts (MoE)**<br>
3140亿参数、混合专家模型

**A machine with enough GPU memory is required**

**Under the Apache 2.0 license**<br>
商用友好
