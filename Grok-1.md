# Grok-1

https://x.ai

**314B parameters Mixture-of-Experts (MoE)**<br>
3140亿参数、混合专家模型

**A machine with enough GPU memory is required**<br>
可能需要一台具有628GB GPU内存的机器（每个参数2字节）<br>
所以可能需要8块H100（每个80GB）

**Under the Apache 2.0 license**<br>
商用友好
